{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "src= \"/home/pallavi/Audio_Speech/Actor_24\"\n",
    "\n",
    "for file in os.listdir(src):\n",
    "    name = file[:file.rfind(\".\")]\n",
    "    #ffmpeg -y -i $filename -map_metadata -1 -codec copy $newfilename\n",
    "    subprocess.call([\"ffmpeg\",\"-y\",\"-i\",src+\"/\"+name+\".wav\",\"-map_metadata\",\"-1\",\"-codec\",\"copy\",src+\"/\"+name+\".wav\"])\n",
    "    #subprocess.call([\"fmpeg\", \"-i\", sourcedir+\"/\"+name+\".avi\", sourcedir+\"/\"+name+\".webm\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "rate, data = scipy.io.wavfile.read(\"/home/pallavi/Audio_Speech/All/03-01-08-01-02-02-01.wav\")\n",
    "print (rate)\n",
    "print(len(data))\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "print(data.shape[0])\n",
    "plt.plot(data[:114688])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "filename=\"03-01-05-01-01-01-24.wav\"\n",
    "rate, data = scipy.io.wavfile.read(\"/home/pallavi/Audio_Speech/Actor_24/\"+filename)\n",
    "emo=filename[7]\n",
    "print(emo)\n",
    "plt.plot(data[:114688])\n",
    "data=data/ (2.**15)\n",
    "plt.plot(data[:114688])\n",
    "print (rate)\n",
    "print(len(data))\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "data=data.tolist()\n",
    "print(data[10])\n",
    "#print(data)\n",
    "a=array(data)\n",
    "print(a.dtype)\n",
    "print(a.shape)\n",
    "plt.plot(data[:114688])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=data[:]\n",
    "timeArray = np.arange(0, 114688, 1)\n",
    "timeArray = timeArray / rate\n",
    "print(timeArray.shape)\n",
    "print(d.shape)\n",
    "timeArray = timeArray * 1000  #scale to milliseconds\n",
    "plt.plot(timeArray, d, color='k')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Time (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "\n",
    "import os\n",
    "path=\"/home/pallavi/Audio_Speech/All\"\n",
    "n=114688\n",
    "arr = np.empty(shape=(0,n))\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    rate, data = scipy.io.wavfile.read(\"/home/pallavi/Audio_Speech/All/\"+filename)\n",
    "    if data.shape[0]==n:\n",
    "        arr=np.append(arr,[data],axis=0)\n",
    "    else:\n",
    "        print(filename)\n",
    "print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "\n",
    "import os\n",
    "path=\"/home/pallavi/Audio_Speech/All2\"\n",
    "n=114688\n",
    "a = np.empty(shape=(0,n))\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    rate, data = scipy.io.wavfile.read(\"/home/pallavi/Audio_Speech/All2/\"+filename)\n",
    "    if data.shape[0]==n:\n",
    "        a=np.append(a,[data],axis=0)\n",
    "    else:\n",
    "        print(filename)\n",
    "print(len(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all files to derive data and save into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "\n",
    "import os\n",
    "path=\"/home/pallavi/Audio_Speech/Allfiles\"\n",
    "n=114688\n",
    "m=1435\n",
    "#arr = np.empty(shape=(0,n-40000))\n",
    "emotion = np.empty([0, 1], dtype=int)\n",
    "e=[]\n",
    "arr=[]\n",
    "for filename in os.listdir(path):\n",
    "    emo=int(filename[7])\n",
    "    rate, data = scipy.io.wavfile.read(\"/home/pallavi/Audio_Speech/Allfiles/\"+filename)\n",
    "    #print(data.shape)\n",
    "    if data.shape[0]==n:\n",
    "        e.append(emo)\n",
    "        data = data[35763:]\n",
    "        data=data.tolist()\n",
    "        arr.append(data)\n",
    "        #emotion=np.append(emotion,emo,axis=0)\n",
    "        #arr=np.append(arr,[data],axis=0)\n",
    "    else:\n",
    "        print(filename)\n",
    "print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((arr.shape))\n",
    "train = arr[:1300]\n",
    "test = arr[1300:]\n",
    "print(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[]\n",
    "i=0\n",
    "for l in arr:\n",
    "    l.append(e[i])\n",
    "    i=i+1\n",
    "    final.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('speech.csv','w') as myfile:\n",
    "    wr = csv.writer(myfile,quoting=csv.QUOTE_NONE)\n",
    "    for f in final:\n",
    "        wr.writerow(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('speech.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import speechpy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_folder = \"home/\"\n",
    "\n",
    "class_labels = [\"Neutral\", \"Angry\", \"Happy\", \"Sad\"]\n",
    "\n",
    "mslen = 100000  # Empirically calculated for the given dataset\n",
    "\n",
    "\n",
    "def read_wav(filename):\n",
    "    \"\"\"\n",
    "    Read the wav file and return corresponding data\n",
    "    :param filename: name of the file\n",
    "    :return: return tuple containing sampling frequency and signal\n",
    "    \"\"\"\n",
    "    return wav.read(filename)\n",
    "\n",
    "\n",
    "def get_data(flatten=True, mfcc_len=39):\n",
    "    \"\"\"\n",
    "    Read the files get the data perform the test-train split and return them to the caller\n",
    "    :param mfcc_len: Number of mfcc features to take for each frame\n",
    "    :param flatten: Boolean specifying whether to flatten the data or not\n",
    "    :return: 4 arrays, x_train x_test y_train y_test\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    max_fs = 0\n",
    "    min_sample = int('9' * 10)\n",
    "    s = 0\n",
    "    cnt = 0\n",
    "    cur_dir = os.getcwd()\n",
    "    os.chdir('..')\n",
    "    os.chdir(dataset_folder)\n",
    "    for i, directory in enumerate(class_labels):\n",
    "        print \"started reading folder\", directory\n",
    "        os.chdir(directory)\n",
    "        for filename in os.listdir('.'):\n",
    "            fs, signal = read_wav(filename)\n",
    "            max_fs = max(max_fs, fs)\n",
    "            s_len = len(signal)\n",
    "            # pad the signals to have same size if lesser than required\n",
    "            # else slice them\n",
    "            if s_len < mslen:\n",
    "                pad_len = mslen - s_len\n",
    "                pad_rem = pad_len % 2\n",
    "                pad_len /= 2\n",
    "                signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values=0)\n",
    "            else:\n",
    "                pad_len = s_len - mslen\n",
    "                pad_rem = pad_len % 2\n",
    "                pad_len /= 2\n",
    "                signal = signal[pad_len:pad_len + mslen]\n",
    "            min_sample = min(len(signal), min_sample)\n",
    "            mfcc = speechpy.feature.mfcc(signal, fs, num_cepstral=mfcc_len)\n",
    "\n",
    "            if flatten:\n",
    "                # Flatten the data\n",
    "                mfcc = mfcc.flatten()\n",
    "            data.append(mfcc)\n",
    "            labels.append(i)\n",
    "            cnt += 1\n",
    "        print \"ended reading folder\", directory\n",
    "        os.chdir('..')\n",
    "    os.chdir(cur_dir)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1435\n",
      "int32\n",
      "(1435, 60000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "from numpy import array\n",
    "import os\n",
    "\n",
    "path=\"C:/Users/Apurv/Psychometric Analysis/Allfiles/\"\n",
    "n=114688\n",
    "m=1435\n",
    "#arr = np.empty(shape=(0,n-40000))\n",
    "#emotion = np.empty([0, 1], dtype=int)\n",
    "e=[]\n",
    "arr=[]\n",
    "app=np.zeros([60000, 1435])\n",
    "i=0\n",
    "for filename in os.listdir(path):\n",
    "    emo=int(filename[7])\n",
    "    rate, data = scipy.io.wavfile.read(\"C:/Users/Apurv/Psychometric Analysis/Allfiles/\"+filename)\n",
    "    if data.shape[0]==n:\n",
    "        e.append(emo)\n",
    "        data = data[54688:]\n",
    "        for j in range(0,60000):\n",
    "            app[j][i]=data[j]\n",
    "        data=data.tolist()\n",
    "        arr.append(data)\n",
    "        #emotion=np.append(emotion,emo,axis=0)\n",
    "        #arr=np.append(arr,[data],axis=0)\n",
    "    else:\n",
    "        print(filename)\n",
    "    i=i+1\n",
    "    \n",
    "print(len(arr))\n",
    "a=array(arr)\n",
    "print(a.dtype)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454.0\n",
      "float64\n",
      "(60000, 1435)\n"
     ]
    }
   ],
   "source": [
    "print(app[6000][105])\n",
    "print(app.dtype)\n",
    "print(app.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1435, 60000)\n",
      "(1435,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "capp=app\n",
    "capp=np.transpose(capp)\n",
    "print(capp.shape)\n",
    "e=array(e)\n",
    "print(e.shape)\n",
    "print(e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apurv\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from keras.utils import to_categorical\n",
    "labels = []\n",
    "for i in range(0,1435):\n",
    "    lab=to_categorical(e[i], 9)\n",
    "    lab=lab[1:]\n",
    "    labels.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1435, 75, 800)\n",
      "float64\n",
      "float64\n",
      "(1435, 8)\n"
     ]
    }
   ],
   "source": [
    "capp = capp.reshape(1435, 75, 800)\n",
    "print(capp.shape)\n",
    "print(capp.dtype)\n",
    "print(capp[90][0][58].dtype)\n",
    "labels=array(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "from numpy import array\n",
    "import os\n",
    "\n",
    "path=\"C:/Users/Apurv/testfiles/\"\n",
    "e=[]\n",
    "arr=[]\n",
    "n=114688\n",
    "app=np.zeros([60000, 32])\n",
    "i=0\n",
    "for filename in os.listdir(path):\n",
    "    emo=int(filename[7])\n",
    "    rate, data = scipy.io.wavfile.read(\"C:/Users/Apurv/testfiles/\"+filename)\n",
    "    if data.shape[0]==n:\n",
    "        e.append(emo)\n",
    "        data = data[54688:]\n",
    "        for j in range(0,60000):\n",
    "            app[j][i]=data[j]\n",
    "        data=data.tolist()\n",
    "        arr.append(data)\n",
    "    else:\n",
    "        print(filename)\n",
    "    i=i+1\n",
    "print(len(arr))\n",
    "a=array(arr)\n",
    "x_test=app\n",
    "x_test=np.transpose(x_test)\n",
    "e=array(e)\n",
    "import sklearn\n",
    "from keras.utils import to_categorical\n",
    "y_test = []\n",
    "for i in range(0,32):\n",
    "    lab=to_categorical(e[i], 9)\n",
    "    lab=lab[1:]\n",
    "    y_test.append(lab)\n",
    "x_test = x_test.reshape(32, 75, 800)\n",
    "y_test=array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "ind_list = [i for i in range(1435)]\n",
    "shuffle(ind_list)\n",
    "train_new  = capp[ind_list,:,:]\n",
    "target_new = labels[ind_list,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  321.    62.  -113. ... -1963. -1341.  -705.]\n",
      " [ -361.   -59.   306. ... -1470. -1454. -1486.]\n",
      " [-1500. -1578. -1824. ...   854.   848.   907.]\n",
      " ...\n",
      " [ 1597.  1551.  1468. ...   970.   850.   831.]\n",
      " [  859.   811.   699. ...   243.   130.    -4.]\n",
      " [  -69.   -19.   -14. ...   210.   311.   370.]]\n",
      "[[  321.    62.  -113. ... -1963. -1341.  -705.]\n",
      " [ -361.   -59.   306. ... -1470. -1454. -1486.]\n",
      " [-1500. -1578. -1824. ...   854.   848.   907.]\n",
      " ...\n",
      " [ 1597.  1551.  1468. ...   970.   850.   831.]\n",
      " [  859.   811.   699. ...   243.   130.    -4.]\n",
      " [  -69.   -19.   -14. ...   210.   311.   370.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[[ 189.  187.  180. ...   16.  -22.  -62.]\n",
      " [-101. -138. -168. ... -204. -199. -193.]\n",
      " [-176. -154. -144. ...  -93.  -86.  -69.]\n",
      " ...\n",
      " [ -22.  -29.  -31. ...  -67.  -68.  -74.]\n",
      " [ -76.  -77.  -74. ...  -31.  -17.    9.]\n",
      " [  -2.  -40.  -27. ...  170.  173.  175.]]\n",
      "[[ 189.  187.  180. ...   16.  -22.  -62.]\n",
      " [-101. -138. -168. ... -204. -199. -193.]\n",
      " [-176. -154. -144. ...  -93.  -86.  -69.]\n",
      " ...\n",
      " [ -22.  -29.  -31. ...  -67.  -68.  -74.]\n",
      " [ -76.  -77.  -74. ...  -31.  -17.    9.]\n",
      " [  -2.  -40.  -27. ...  170.  173.  175.]]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[[ 440.  436.  411. ...  -51. -106. -181.]\n",
      " [-130. -121. -182. ...  443.  487.  468.]\n",
      " [ 523.  568.  527. ... -316. -306. -321.]\n",
      " ...\n",
      " [ 249.  257.  259. ... -413. -410. -406.]\n",
      " [-403. -401. -394. ...   34.   23.   17.]\n",
      " [  10.    6.    5. ...  229.  228.  227.]]\n",
      "[[ 440.  436.  411. ...  -51. -106. -181.]\n",
      " [-130. -121. -182. ...  443.  487.  468.]\n",
      " [ 523.  568.  527. ... -316. -306. -321.]\n",
      " ...\n",
      " [ 249.  257.  259. ... -413. -410. -406.]\n",
      " [-403. -401. -394. ...   34.   23.   17.]\n",
      " [  10.    6.    5. ...  229.  228.  227.]]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(capp[1360])\n",
    "print(train_new[0])\n",
    "print(labels[1360])\n",
    "print(target_new[0])\n",
    "\n",
    "print(capp[273])\n",
    "print(train_new[1])\n",
    "print(labels[273])\n",
    "print(target_new[1])\n",
    "\n",
    "print(capp[385])\n",
    "print(train_new[4])\n",
    "print(labels[385])\n",
    "print(target_new[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_test, np.array(y_test), verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', 100*score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1291 samples, validate on 144 samples\n",
      "Epoch 1/5\n",
      "1291/1291 [==============================] - 27s 21ms/step - loss: 2.0628 - acc: 0.1472 - val_loss: 2.0234 - val_acc: 0.2639\n",
      "Epoch 2/5\n",
      "1291/1291 [==============================] - 22s 17ms/step - loss: 1.9715 - acc: 0.2517 - val_loss: 1.8868 - val_acc: 0.2778\n",
      "Epoch 3/5\n",
      "1291/1291 [==============================] - 22s 17ms/step - loss: 1.8538 - acc: 0.3036 - val_loss: 1.8148 - val_acc: 0.2778\n",
      "Epoch 4/5\n",
      "1291/1291 [==============================] - 23s 18ms/step - loss: 1.7506 - acc: 0.3431 - val_loss: 1.8832 - val_acc: 0.3194\n",
      "Epoch 5/5\n",
      "1291/1291 [==============================] - 22s 17ms/step - loss: 1.6586 - acc: 0.3803 - val_loss: 1.9127 - val_acc: 0.3056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29c5bb44160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "modelser = Sequential()\n",
    "\n",
    "#LSTM Layer1\n",
    "modelser.add(LSTM(256, return_sequences=True, input_shape=(75, 800)))     # returns a sequence of vectors of dimension 32\n",
    "\n",
    "#LSTM Layer2\n",
    "modelser.add(LSTM(128, return_sequences=True))                            # returns a sequence of vectors of dimension 32\n",
    "\n",
    "#LSTM Layer3\n",
    "modelser.add(LSTM(64))                                                    # return a single vector of dimension 32\n",
    "\n",
    "#Hidden Layer\n",
    "modelser.add(Dense(32, activation='relu'))\n",
    "\n",
    "#Output layer\n",
    "modelser.add(Dense(8, activation='softmax'))\n",
    "\n",
    "modelser.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modelser.fit(train_new, target_new, batch_size=128, epochs=5, verbose=1, validation_split=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "modelser_json = modelser.to_json()\n",
    "with open(\"modelser.json\", \"w\") as json_file:\n",
    "    json_file.write(modelser_json)\n",
    "# serialize weights to HDF5\n",
    "modelser.save_weights(\"modelser.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('modelser.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"modelser.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "rate, data = scipy.io.wavfile.read(\"RECORDING.wav\")\n",
    "print (rate)\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "new=[]\n",
    "for b in data:\n",
    "    b=max(b[0],b[1])\n",
    "    if b>0:\n",
    "        new.append(b)\n",
    "print(len(new))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=np.array(new)\n",
    "print(new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(new)\n",
    "c=int(l/60000)\n",
    "rec=[]\n",
    "i=0\n",
    "for i in range(0,c):\n",
    "    rec.append(new[60000*i:60000*(i+1)])\n",
    "rec=np.array(rec)    \n",
    "print(rec.shape)\n",
    "print(rec[2])\n",
    "#rec=rec.reshape(c,40,1500)\n",
    "for i in range(0,c):\n",
    "    find = rec[i]\n",
    "    find=find.reshape(1,75,800)\n",
    "    custom = loaded_model.predict(find)\n",
    "    percent_emot = custom[0]\n",
    "    output_emot = custom[0].tolist()\n",
    "    disp_output = \"Probability assigned to each emotion class as percentage:\" + \"\\n\\nNeutral: \"+ str(output_emot[0]*100)+ \"\\n\\nCalm: \"+ str(output_emot[1]*100)+ \"\\n\\nHappy: \"+ str(output_emot[2]*100)+ \"\\n\\nSad: \"+ str(output_emot[3]*100) + \"\\n\\nAngry: \"+ str(output_emot[4]*100) + \"\\n\\nFearful: \"+ str(output_emot[5]*100) + \"\\n\\nDisgust: \" +str(output_emot[6]*100 )+\"\\n\\nSurprised: \" + str(output_emot[7]*100) \n",
    "    print(disp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=new[0:-l]\n",
    "if len(new)==60001:\n",
    "    new=new[1:]\n",
    "print(len(new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new.shape)\n",
    "new=new.reshape(1,40,1500)\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "from numpy import array\n",
    "import os\n",
    "\n",
    "path=\"C:/Users/Apurv/testfiles/\"\n",
    "e=[]\n",
    "arr=[]\n",
    "n=114688\n",
    "app=np.zeros([60000, 32])\n",
    "i=0\n",
    "for filename in os.listdir(path):\n",
    "    emo=int(filename[7])\n",
    "    rate, data = scipy.io.wavfile.read(\"C:/Users/Apurv/testfiles/\"+filename)\n",
    "    if data.shape[0]==n:\n",
    "        e.append(emo)\n",
    "        data = data[54688:]\n",
    "        for j in range(0,60000):\n",
    "            app[j][i]=data[j]\n",
    "        data=data.tolist()\n",
    "        arr.append(data)\n",
    "    else:\n",
    "        print(filename)\n",
    "    i=i+1\n",
    "print(len(arr))\n",
    "a=array(arr)\n",
    "x_test=app\n",
    "x_test=np.transpose(x_test)\n",
    "e=array(e)\n",
    "import sklearn\n",
    "from keras.utils import to_categorical\n",
    "y_test = []\n",
    "for i in range(0,32):\n",
    "    lab=to_categorical(e[i], 9)\n",
    "    lab=lab[1:]\n",
    "    y_test.append(lab)\n",
    "x_test = x_test.reshape(32, 75, 800)\n",
    "y_test=array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_test, np.array(y_test), verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', 100*score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = loaded_model.predict(new)\n",
    "percent_emot = custom[0]\n",
    "print(custom[0])\n",
    "output_emot = custom[0].tolist();\n",
    "disp_output = \"Probability assigned to each emotion class as percentage:\" + \"\\n\\nNeutral: \"+ str(output_emot[0]*100)+ \"\\n\\nCalm: \"+ str(output_emot[1]*100)+ \"\\n\\nHappy: \"+ str(output_emot[2]*100)+ \"\\n\\nSad: \"+ str(output_emot[3]*100) + \"\\n\\nAngry: \"+ str(output_emot[4]*100) + \"\\n\\nFearful: \"+ str(output_emot[5]*100) + \"\\n\\nDisgust: \" +str(output_emot[6]*100 )+\"\\n\\nSurprised: \" + str(output_emot[7]*100) \n",
    "print(disp_output)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,activation='sigmoid',batch_input_shape=(1435,1,60000)))\n",
    "model.add(Dense(9))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "model.fit(capp, labels, epochs=5, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
